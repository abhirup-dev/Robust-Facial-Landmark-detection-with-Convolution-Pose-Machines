{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.utils.data as data\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import datasets.GeneralDataset as GenDataset\n",
    "import time\n",
    "from san_vision import transforms\n",
    "import os.path as osp\n",
    "from utils import AverageMeter\n",
    "from model import ITN_CPM\n",
    "from model import np2variable, variable2np\n",
    "import torchvision.transforms as TT\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global params\n",
    "params = {\n",
    "    'num_pts': 68,\n",
    "    'convert68to49' : False,\n",
    "    'path': 'xx', \n",
    "    'argmax_radius' : 1,\n",
    "    'downsample' : 8,\n",
    "    'batch_size' : 20,\n",
    "    'heatmap_type' : 'gaussian',\n",
    "    'dataset_name' : '300W/Original(GTB)',\n",
    "    'momentum' : 0.9,\n",
    "    'learning_rate' : 0.00010,\n",
    "    'decay' : 0.0005,\n",
    "    'total_epochs' : 100,\n",
    "    'crop_width' : 256,\n",
    "    'crop_height' : 256,\n",
    "    'pre_crop_expand' : 0.2,\n",
    "    'crop_perturb_max' : 30,\n",
    "    'scale_prob' : 1.1,\n",
    "    'scale_max' : 1,\n",
    "    'scale_min' : 1,\n",
    "    'scale_eval' : 1,\n",
    "    'sigma' : 4,\n",
    "    'train_list' : '/home/abhirup/Datasets/300W-Style/box-coords/300W-Original/300w.train.GTB',\n",
    "    'resume' : 'checkpoint.pth.tar'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def weights_init_cpm(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0, 0.01)\n",
    "        if m.bias is not None: m.bias.data.zero_()\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def remove_module_dict(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "def load_weight_from_dict(model, weight_state_dict, param_pair=None, remove_prefix=True):\n",
    "    if remove_prefix: weight_state_dict = remove_module_dict(weight_state_dict)\n",
    "    all_parameter = model.state_dict()\n",
    "    all_weights   = []\n",
    "    finetuned_layer, random_initial_layer = [], []\n",
    "    for key, value in all_parameter.items():\n",
    "        if param_pair is not None and key in param_pair:\n",
    "            all_weights.append((key, weight_state_dict[ param_pair[key] ]))\n",
    "        elif key in weight_state_dict:\n",
    "            all_weights.append((key, weight_state_dict[key]))\n",
    "            finetuned_layer.append(key)\n",
    "        else:\n",
    "            all_weights.append((key, value))\n",
    "            random_initial_layer.append(key)\n",
    "#     print ('==>[load_model] finetuned layers : {}'.format(finetuned_layer))\n",
    "#     print ('==>[load_model] keeped layers : {}'.format(random_initial_layer))\n",
    "    all_weights = OrderedDict(all_weights)\n",
    "    model.load_state_dict(all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(criterion, target_var, outputs, mask_var, total_labeled_cpm):\n",
    "    total_loss = 0\n",
    "    each_stage_loss = []\n",
    "    mask_outputs = []\n",
    "    for output_var in outputs:\n",
    "        stage_loss = 0\n",
    "        output = torch.masked_select(output_var, mask_var)\n",
    "        target = torch.masked_select(target_var, mask_var)\n",
    "    #     print(output.size(), target.size())\n",
    "        mask_outputs.append(output)\n",
    "\n",
    "        stage_loss = criterion(output, target)/(total_labeled_cpm*2)\n",
    "        total_loss += stage_loss\n",
    "        each_stage_loss.append(stage_loss.item())\n",
    "    return total_loss, each_stage_loss\n",
    "# mask_var[:,:num_pts,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fill   = tuple( [int(x*255) for x in [0.5, 0.5, 0.5] ] )\n",
    "normalize   = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                      std=[0.5, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = [transforms.PreCrop(params['pre_crop_expand'])]\n",
    "train_transform = [transforms.TrainScale2WH((params['crop_width'], params['crop_height']))]\n",
    "train_transform += [transforms.AugScale(params['scale_prob'], params['scale_min'], params['scale_max'])]\n",
    "train_transform += [transforms.AugCrop(params['crop_width'], params['crop_height'], params['crop_perturb_max'], mean_fill)]\n",
    "train_transform += [transforms.ToTensor(), normalize]\n",
    "train_transform = transforms.Compose( train_transform )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The general dataset initialization done, sigma is 4, downsample is 8, dataset-name : 300W/Original(GTB), self is : GeneralDataset(number of point=-1, heatmap_type=gaussian)\n",
      "Load list from /home/abhirup/Datasets/300W-Style/box-coords/300W-Original/300w.train.GTB\n",
      "Load [0/1]-th list : /home/abhirup/Datasets/300W-Style/box-coords/300W-Original/300w.train.GTB with 3148 images\n",
      "Start load data for the general datas\n",
      "Load data done for the general dataset, which has 3148 images.\n"
     ]
    }
   ],
   "source": [
    "train_data = GenDataset(train_transform, params['sigma'], params['downsample'], params['heatmap_type'], params['dataset_name'])\n",
    "train_data.load_list(params['train_list'], params['num_pts'], True)\n",
    "# if(params['convert68to49']): train_data.convert68to49()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_transform  = transforms.Compose([transforms.PreCrop(params['pre_crop_expand']),\n",
    "#                                       transforms.TrainScale2WH((params['crop_width'],params['crop_height'])),\n",
    "#                                       transforms.ToTensor(), normalize])\n",
    "\n",
    "# eval_data = GenDataset(eval_transform, params['sigma'], params['downsample'], params['heatmap_type'], params['dataset_name'])\n",
    "# eval_data.load_list('/home/abhirup/Datasets/300W-Style/box-coords/300W-Original/test',\n",
    "#                    68, True)\n",
    "# eval_loader = data.DataLoader(eval_data, batch_size=5, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurs, No graph saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='./log')\n",
    "net = ITN_CPM(params)\n",
    "writer.add_graph(net)\n",
    "net.apply(weights_init_cpm)\n",
    "# net_param_dict = net.parameters()\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(False)\n",
    "criterion.cuda()\n",
    "optimizer = torch.optim.Adam(\n",
    "#                             net.parameters(), lr=params['learning_rate'],\n",
    "                            net.specify_parameter(base_lr=params['learning_rate'], \n",
    "                                                  base_weight_decay=params['decay']),  amsgrad=False)\n",
    "#                             momentum=params['momentum'],\n",
    "#                             nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.75**(epoch//5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'checkpoint.pth.tar'\n",
      "=> loaded checkpoint 'checkpoint.pth.tar' (epoch 27) current_loss = 19.98334503173828\n"
     ]
    }
   ],
   "source": [
    "start_epoch=0\n",
    "best_loss=99999\n",
    "if (params['resume'] and osp.isfile(params['resume'])):\n",
    "    print(\"=> loading checkpoint '{}'\".format(params['resume']))\n",
    "    checkpoint = torch.load(params['resume'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {}) current_loss = {}\"\n",
    "          .format(params['resume'], checkpoint['epoch'], best_loss))\n",
    "else:\n",
    "    model_urls = 'http://download.pytorch.org/models/vgg16-397923af.pth'\n",
    "    weights = model_zoo.load_url(model_urls)\n",
    "    load_weight_from_dict(net, weights, None, False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_data, batch_size=params['batch_size'], shuffle=True, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_time = AverageMeter()\n",
    "data_time = AverageMeter()\n",
    "forward_time = AverageMeter()\n",
    "visible_points = AverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:27, Now learning rate:[2.3730468750000002e-05, 4.7460937500000004e-05, 2.3730468750000002e-05, 4.7460937500000004e-05, 2.3730468750000002e-05, 4.7460937500000004e-05, 9.492187500000001e-05, 0.00018984375000000002, 9.492187500000001e-05, 0.00018984375000000002]\n",
      "batch_iter:0, total_loss:14.886838912963867, stage_losses:7.192742347717285, 3.745077610015869, 3.9490184783935547\n",
      "batch_iter:1, total_loss:29.446449279785156, stage_losses:7.339168071746826, 10.28019905090332, 11.827082633972168\n",
      "batch_iter:2, total_loss:24.208053588867188, stage_losses:10.54419231414795, 7.289914608001709, 6.373946666717529\n",
      "batch_iter:3, total_loss:21.671680450439453, stage_losses:6.635532379150391, 7.9913763999938965, 7.044771671295166\n",
      "batch_iter:4, total_loss:27.40978240966797, stage_losses:8.574284553527832, 9.302638053894043, 9.532858848571777\n",
      "batch_iter:5, total_loss:23.738903045654297, stage_losses:9.726409912109375, 6.585782051086426, 7.426711559295654\n",
      "batch_iter:6, total_loss:26.554901123046875, stage_losses:11.604257583618164, 7.447000026702881, 7.503642559051514\n",
      "batch_iter:7, total_loss:23.62875747680664, stage_losses:11.484517097473145, 6.3099446296691895, 5.834295272827148\n",
      "batch_iter:8, total_loss:17.612823486328125, stage_losses:7.028615474700928, 5.772651672363281, 4.811555862426758\n",
      "batch_iter:9, total_loss:23.717544555664062, stage_losses:9.955461502075195, 6.8322930335998535, 6.929791450500488\n",
      "batch_iter:10, total_loss:18.676082611083984, stage_losses:8.896627426147461, 4.752286434173584, 5.027167797088623\n",
      "batch_iter:11, total_loss:23.864322662353516, stage_losses:10.750067710876465, 6.401342868804932, 6.712912082672119\n",
      "batch_iter:12, total_loss:16.802087783813477, stage_losses:7.201930999755859, 4.945825099945068, 4.654330730438232\n",
      "batch_iter:13, total_loss:18.20901870727539, stage_losses:8.648058891296387, 5.013978481292725, 4.546981334686279\n",
      "batch_iter:14, total_loss:19.810640335083008, stage_losses:8.778186798095703, 5.56409215927124, 5.468360900878906\n",
      "batch_iter:15, total_loss:17.869457244873047, stage_losses:7.413567543029785, 5.253880977630615, 5.202010154724121\n",
      "batch_iter:16, total_loss:17.076047897338867, stage_losses:7.799698829650879, 4.623607158660889, 4.6527419090271\n",
      "batch_iter:17, total_loss:17.839187622070312, stage_losses:8.153732299804688, 4.811102390289307, 4.874353885650635\n",
      "batch_iter:18, total_loss:18.399497985839844, stage_losses:8.456856727600098, 4.907196998596191, 5.035443305969238\n",
      "batch_iter:19, total_loss:14.656982421875, stage_losses:6.798193454742432, 3.890458822250366, 3.968330144882202\n",
      "batch_iter:20, total_loss:15.131778717041016, stage_losses:7.049844264984131, 3.9819085597991943, 4.1000261306762695\n",
      "batch_iter:21, total_loss:15.959464073181152, stage_losses:8.224105834960938, 3.7426934242248535, 3.9926655292510986\n",
      "batch_iter:22, total_loss:13.027688026428223, stage_losses:5.767967224121094, 3.5151264667510986, 3.7445945739746094\n",
      "batch_iter:23, total_loss:16.374897003173828, stage_losses:7.367859840393066, 4.352006435394287, 4.655030250549316\n",
      "batch_iter:24, total_loss:13.948324203491211, stage_losses:6.1536335945129395, 3.700989246368408, 4.0937018394470215\n",
      "batch_iter:25, total_loss:14.146203994750977, stage_losses:6.323266506195068, 3.7833316326141357, 4.03960657119751\n",
      "batch_iter:26, total_loss:12.895709991455078, stage_losses:5.847575664520264, 3.4896130561828613, 3.5585217475891113\n",
      "batch_iter:27, total_loss:21.40425682067871, stage_losses:8.619518280029297, 6.1476311683654785, 6.6371073722839355\n",
      "batch_iter:28, total_loss:16.121017456054688, stage_losses:7.114329814910889, 4.417263031005859, 4.589425086975098\n",
      "batch_iter:29, total_loss:14.8543701171875, stage_losses:6.442465305328369, 4.1063151359558105, 4.305589199066162\n"
     ]
    }
   ],
   "source": [
    "# start_epoch = 1\n",
    "end = time.time()\n",
    "for epoch in range(start_epoch, params['total_epochs']):\n",
    "    scheduler.step(epoch)\n",
    "    print('Epoch:{}, Now learning rate:{}'.format(epoch, scheduler.get_lr()))\n",
    "    for i, (inputs, target, mask, points, image_index, label_sign) in enumerate(train_loader):\n",
    "        # inputs : Batch, Squence, Channel, Height, Width\n",
    "        # data prepare\n",
    "        target = target.cuda(async=True)\n",
    "        # get the real mask\n",
    "        mask.masked_scatter_((1-label_sign).unsqueeze(-1).unsqueeze(-1), torch.ByteTensor(mask.size()).zero_())\n",
    "        mask_var   = mask.cuda(async=True)\n",
    "\n",
    "        batch_size, num_pts = inputs.size(0), mask.size(1)-1\n",
    "        image_index = variable2np(image_index).squeeze(1).tolist()\n",
    "        # check the label indicator, whether is has annotation or not\n",
    "        sign_list = variable2np(label_sign).astype('bool').squeeze(1).tolist()\n",
    "        data_time.update(time.time() - end)\n",
    "        cvisible_points = torch.sum(mask[:,:-1,:,:]) * 1. / batch_size\n",
    "        visible_points.update(cvisible_points, batch_size)\n",
    "\n",
    "        batch_cpms, batch_locs, batch_scos = net(inputs.cuda())\n",
    "\n",
    "#         forward_time.update(time.time() - end)\n",
    "\n",
    "        total_labeled_cpm = int(np.sum(sign_list))\n",
    "\n",
    "        cpm_loss, each_stage_loss_values = compute_loss(criterion, target, batch_cpms, mask_var, total_labeled_cpm)\n",
    "        writer.add_scalars('Loss', {'total loss':cpm_loss, \n",
    "                                    'stage1 loss': each_stage_loss_values[0],\n",
    "                                    'stage2 loss': each_stage_loss_values[1],\n",
    "                                    'stage3 loss': each_stage_loss_values[2]}, epoch*209+i)\n",
    "        for name, param in net.named_parameters():\n",
    "            writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch*209+i)\n",
    "        #passing 'weight_of_idt' or identity loss as None\n",
    "        optimizer.zero_grad()\n",
    "        cpm_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (cpm_loss < best_loss + 5 and i%10==0):\n",
    "            best_loss = cpm_loss\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            })\n",
    "        print(\"batch_iter:{}, total_loss:{}, stage_losses:{}, {}, {}\".format(i, cpm_loss, each_stage_loss_values[0], each_stage_loss_values[1], each_stage_loss_values[2]))\n",
    "    print(\"End of {}th epoch\\n\".format(epoch))\n",
    "#     if cpm_loss < best_loss:\n",
    "#             save_checkpoint({\n",
    "#                 'epoch': epoch,\n",
    "#                 'state_dict': net.state_dict(),\n",
    "#                 'best_loss': best_loss,\n",
    "#                 'optimizer' : optimizer.state_dict(),\n",
    "#             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = TT.ToPILImage()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, (inputs, target, mask, points, image_index, label_sign) in enumerate(eval_loader):\n",
    "    images = [trans(x) for x in inputs]\n",
    "    _, locs, _ = net(inputs.cuda())\n",
    "    for j in range(len(images)):\n",
    "        im = images[j]\n",
    "        loc = locs[j].cpu().data.numpy()\n",
    "        plt.imshow(im)\n",
    "        plt.scatter(loc[:params['num_pts'],0], loc[:params['num_pts'],1], s=8, c='red')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
